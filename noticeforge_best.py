# -*- coding: utf-8 -*-
"""
NoticeForge Core Logic v5.0 (Ultimate: DocuWorks/Excel-MD/LongPath/Binder)
"""
from __future__ import annotations
import os, sys, re, json, time, hashlib, csv, subprocess, html as _html
from dataclasses import dataclass, asdict
from typing import Dict, List, Tuple, Optional, Callable

# Tesseractã®è¨­å®š (Windowsã®ä¸€èˆ¬çš„ãªãƒ‘ã‚¹)
TESSERACT_CMD = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

try:
    import fitz  # PyMuPDF
except Exception:
    fitz = None

try:
    import pytesseract
    from PIL import Image
    pytesseract.pytesseract.tesseract_cmd = TESSERACT_CMD
    TESSERACT_AVAILABLE = True
except Exception:
    TESSERACT_AVAILABLE = False

try:
    from docx import Document
except Exception:
    Document = None

try:
    import openpyxl
    from openpyxl.styles import Font, Alignment
    from openpyxl.utils import get_column_letter
except Exception:
    openpyxl = None

try:
    import xlrd
except Exception:
    xlrd = None

try:
    import xdwlib
    XDWLIB_AVAILABLE = True
except Exception:
    XDWLIB_AVAILABLE = False

# xdw2text.exe ã®å€™è£œãƒ‘ã‚¹ï¼ˆDocuWorksã®ä¸€èˆ¬çš„ãªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å…ˆã‚’ç¶²ç¾…ï¼‰
XDW2TEXT_CANDIDATES = [
    "xdw2text",  # PATHä¸Šã«ã‚ã‚‹å ´åˆ
    r"C:\Program Files\Fuji Xerox\DocuWorks\xdw2text.exe",
    r"C:\Program Files (x86)\Fuji Xerox\DocuWorks\xdw2text.exe",
    r"C:\Program Files\FUJIFILM\DocuWorks\xdw2text.exe",
    r"C:\Program Files (x86)\FUJIFILM\DocuWorks\xdw2text.exe",
    r"C:\Program Files\DocuWorks\xdw2text.exe",
    r"C:\Program Files (x86)\DocuWorks\xdw2text.exe",
]

DEFAULTS: Dict[str, object] = {
    "min_chars_mainbody": 400, # åŸºæº–ã‚’å°‘ã—ç”˜ãã—ã¦æŠ½å‡ºæ¼ã‚Œã‚’é˜²æ­¢
    "max_depth": 30,
    "summary_chars": 900,
    "main_attach_split_keywords": [r"^\s*åˆ¥æ·»", r"^\s*åˆ¥ç´™", r"^\s*ã€åˆ¥æ·»ã€‘", r"^\s*ã€åˆ¥ç´™ã€‘", r"^\s*ã€å‚è€ƒã€‘", r"^\s*è¨˜\s*$"],
    "bind_bytes_limit": 15 * 1024 * 1024,
    "use_ocr": False,
}

FACILITY_TAGS: Dict[str, List[str]] = {
    "è£½é€ æ‰€": [r"è£½é€ æ‰€"],
    "å±‹å¤–ã‚¿ãƒ³ã‚¯è²¯è”µæ‰€": [r"å±‹å¤–ã‚¿ãƒ³ã‚¯è²¯è”µæ‰€", r"æµ®å±‹æ ¹", r"å›ºå®šå±‹æ ¹", r"ã‚¢ãƒ‹ãƒ¥ãƒ©", r"ã‚¿ãƒ³ã‚¯åº•", r"æ³¡æ”¾å°„", r"é˜²æ²¹å ¤"],
    "å±‹å†…è²¯è”µæ‰€": [r"å±‹å†…è²¯è”µæ‰€"],
    "åœ°ä¸‹ã‚¿ãƒ³ã‚¯è²¯è”µæ‰€": [r"åœ°ä¸‹ã‚¿ãƒ³ã‚¯è²¯è”µæ‰€", r"FRPã‚¿ãƒ³ã‚¯", r"æ¼ãˆã„æ¤œçŸ¥"],
    "ç°¡æ˜“ã‚¿ãƒ³ã‚¯è²¯è”µæ‰€": [r"ç°¡æ˜“ã‚¿ãƒ³ã‚¯è²¯è”µæ‰€"],
    "ç§»å‹•ã‚¿ãƒ³ã‚¯è²¯è”µæ‰€": [r"ç§»å‹•ã‚¿ãƒ³ã‚¯è²¯è”µæ‰€", r"ã‚¿ãƒ³ã‚¯ãƒ­ãƒ¼ãƒªãƒ¼"],
    "çµ¦æ²¹å–æ‰±æ‰€": [r"çµ¦æ²¹å–æ‰±æ‰€", r"è¨ˆé‡æ©Ÿ", r"ãƒã‚ºãƒ«", r"\bSS\b", r"ã‚µãƒ¼ãƒ“ã‚¹ã‚¹ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³"],
    "è²©å£²å–æ‰±æ‰€": [r"è²©å£²å–æ‰±æ‰€"],
    "ç§»é€å–æ‰±æ‰€": [r"ç§»é€å–æ‰±æ‰€", r"è·å¸ã—", r"è·ç©ã¿"],
    "ä¸€èˆ¬å–æ‰±æ‰€": [r"ä¸€èˆ¬å–æ‰±æ‰€", r"å¡—è£…", r"æ´—æµ„", r"æ··åˆ", r"å……å¡«", r"ä¹¾ç‡¥"],
    "å…±é€š": [r"å±é™ºç‰©", r"æ¶ˆé˜²æ³•", r"æ”¿ä»¤", r"è¦å‰‡", r"é‹ç”¨", r"å–æ‰±ã„", r"è³ªç–‘", r"Q&A", r"è§£é‡ˆ"],
}

WORK_TAGS: Dict[str, List[str]] = {
    "ç”³è«‹ãƒ»å±Šå‡º": [r"è¨±å¯", r"å±Šå‡º", r"ç”³è«‹", r"å¤‰æ›´", r"ä»®ä½¿ç”¨", r"å®Œæˆæ¤œæŸ»", r"äºˆé˜²è¦ç¨‹", r"æ‰¿èª", r"å±Šæ›¸", r"æ§˜å¼"],
    "æŠ€è¡“åŸºæº–ãƒ»è¨­å‚™": [r"æŠ€è¡“åŸºæº–", r"åŸºæº–", r"æ§‹é€ ", r"è¨­å‚™", r"é…ç®¡", r"ã‚¿ãƒ³ã‚¯", r"ä¿æœ‰ç©ºåœ°", r"è€éœ‡", r"è…é£Ÿ", r"æ¼ãˆã„æ¤œçŸ¥"],
    "é‹ç”¨è§£é‡ˆãƒ»Q&A": [r"å–æ‰±ã„", r"é‹ç”¨", r"è§£é‡ˆ", r"è³ªç–‘", r"å•", r"ç­”", r"Q&A", r"ç…§ä¼š", r"å›ç­”"],
    "äº‹æ•…ãƒ»æ¼ãˆã„ãƒ»ç«ç½": [r"äº‹æ•…", r"æ¼ãˆã„", r"æµå‡º", r"ç«ç½", r"çˆ†ç™º", r"ç½å®³", r"åŸå› ", r"å†ç™ºé˜²æ­¢"],
    "æ¶ˆç«ãƒ»é˜²ç½": [r"æ³¡", r"æ¶ˆç«", r"å›ºå®šæ¶ˆç«", r"è­¦å ±", r"ç·Šæ€¥é®æ–­", r"é¿é›£", r"é˜²ç½", r"æ¶ˆç«è¨­å‚™"],
    "ç«‹å…¥æ¤œæŸ»ãƒ»æŒ‡å°": [r"ç«‹å…¥", r"æ¤œæŸ»", r"æŒ‡å°", r"æ˜¯æ­£", r"æ”¹å–„", r"ç¢ºèª", r"ç‚¹æ¤œ", r"å ±å‘Š"],
    "æ•™è‚²ãƒ»ä½“åˆ¶": [r"ä¿å®‰ç›£ç£", r"å±é™ºç‰©ä¿å®‰ç›£ç£è€…", r"ä¿å®‰çµ±æ‹¬", r"æ•™è‚²", r"è¨“ç·´", r"ä½“åˆ¶", r"è²¬ä»»è€…"],
}

@dataclass
class Record:
    relpath: str
    ext: str
    size: int
    mtime: float
    sha1: str
    method: str
    pages: Optional[int]
    text_chars: int
    needs_review: bool
    reason: str
    title_guess: str
    date_guess: str
    issuer_guess: str
    summary: str
    tags_facility: List[str]
    tags_work: List[str]
    tag_evidence: Dict[str, List[str]]
    out_txt: str
    full_text_for_bind: str = ""

def get_safe_path(path: str) -> str:
    """Windowsã®260æ–‡å­—åˆ¶é™(MAX_PATH)ã‚’çªç ´ã™ã‚‹ãŸã‚ã®å®‰å…¨ãªãƒ‘ã‚¹å¤‰æ›"""
    abs_path = os.path.abspath(path)
    if sys.platform.startswith("win") and not abs_path.startswith("\\\\?\\"):
        return "\\\\?\\" + abs_path
    return abs_path

def extract_pdf(path: str, use_ocr: bool) -> Tuple[str, Optional[int], str]:
    if not fitz: return "", None, "pymupdf_missing"
    text_parts = []
    method = "pdf_text"
    try:
        doc = fitz.open(get_safe_path(path))
        pages = doc.page_count
        for i in range(pages):
            page = doc.load_page(i)
            page_text = page.get_text("text") or ""
            if use_ocr and len(page_text.strip()) < 50 and TESSERACT_AVAILABLE:
                try:
                    pix = page.get_pixmap(dpi=200)
                    img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
                    ocr_text = pytesseract.image_to_string(img, lang="jpn")
                    ocr_text = re.sub(r'([ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥])\s+([ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥])', r'\1\2', ocr_text)
                    page_text += "\n" + ocr_text
                    method = "pdf_ocr"
                except Exception:
                    pass
            text_parts.append(page_text)
        doc.close()
        return "\n".join(text_parts), pages, method
    except Exception as e:
        return "", None, f"pdf_err:{e.__class__.__name__}"

def extract_docx(path: str) -> Tuple[str, str]:
    if not Document: return "", "docx_missing"
    try:
        doc = Document(get_safe_path(path))
        parts = [p.text for p in doc.paragraphs if p.text.strip()]
        for table in doc.tables:
            for row in table.rows:
                cells = [cell.text.strip().replace("\n", " ") for cell in row.cells]
                if any(cells):
                    parts.append("| " + " | ".join(cells) + " |")
        return "\n".join(parts), "docx_text"
    except Exception as e:
        return "", f"docx_err:{e.__class__.__name__}"

def extract_excel(path: str) -> Tuple[str, str]:
    """æ–°æ—§ã‚¨ã‚¯ã‚»ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€AIãŒç†è§£ã—ã‚„ã™ã„Markdownè¡¨å½¢å¼ã«æ•´å½¢ã™ã‚‹"""
    out = []
    ext = os.path.splitext(path)[1].lower()
    safe_p = get_safe_path(path)
    try:
        if ext in (".xlsx", ".xlsm") and openpyxl:
            wb = openpyxl.load_workbook(safe_p, data_only=True, read_only=True)
            for ws in wb.worksheets[:10]:
                out.append(f"## Sheet: {ws.title}")
                for row in ws.iter_rows(max_row=400, max_col=40, values_only=True):
                    if any(row):
                        out.append("| " + " | ".join([str(c).strip().replace("\n", " ") if c is not None else "" for c in row]) + " |")
                out.append("")
            wb.close()
            return "\n".join(out), "xlsx_md"
        elif ext == ".xls" and xlrd:
            wb = xlrd.open_workbook(safe_p)
            for sheet_idx in range(min(10, wb.nsheets)):
                ws = wb.sheet_by_index(sheet_idx)
                out.append(f"## Sheet: {ws.name}")
                for row_idx in range(min(400, ws.nrows)):
                    row = ws.row_values(row_idx)
                    if any(row):
                        out.append("| " + " | ".join([str(c).strip().replace("\n", " ") if c else "" for c in row]) + " |")
                out.append("")
            return "\n".join(out), "xls_md"
        else:
            return "", "excel_lib_missing"
    except Exception as e:
        return "", f"excel_err:{e.__class__.__name__}"

def extract_xdw(path: str) -> Tuple[str, str]:
    """DocuWorksã‹ã‚‰ç›´æ¥ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆxdwlibå„ªå…ˆã€æ¬¡ã«xdw2textè¤‡æ•°ãƒ‘ã‚¹è©¦è¡Œï¼‰"""
    safe_p = get_safe_path(path)

    # æ–¹æ³•1: xdwlibï¼ˆPythonè£½DocuWorksãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ï¼‰ã‚’å„ªå…ˆçš„ã«è©¦ã™
    if XDWLIB_AVAILABLE:
        try:
            doc = xdwlib.xdwopen(path)
            texts = []
            for pg in range(doc.pages):
                page = doc[pg]
                texts.append(page.text)
            doc.close()
            result = "\n".join(texts)
            if result.strip():
                return result, "xdw_xdwlib"
        except Exception:
            pass  # å¤±æ•—ã—ãŸã‚‰xdw2textã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

    # æ–¹æ³•2: xdw2text.exe ã‚’è¤‡æ•°ã®å€™è£œãƒ‘ã‚¹ã§è©¦ã™
    for cmd in XDW2TEXT_CANDIDATES:
        try:
            result = subprocess.run(
                [cmd, safe_p],
                capture_output=True, text=True,
                encoding="cp932", errors="ignore",
                timeout=30
            )
            if result.returncode == 0 and result.stdout.strip():
                return result.stdout, "xdw_text"
        except FileNotFoundError:
            continue  # ã“ã®ãƒ‘ã‚¹ã«ã¯exeãŒãªã„ã®ã§æ¬¡ã‚’è©¦ã™
        except Exception:
            continue

    return "", "xdw2text_missing (è¦xdw2text.exeå°å…¥: DocuWorksã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ•ã‚©ãƒ«ãƒ€å†…)"

def split_main_attach(text: str, kws: List[str]) -> Tuple[str, str]:
    lines = text.splitlines()
    cut_idx = -1
    for i, line in enumerate(lines):
        for k in kws:
            if re.match(k, line):
                cut_idx = i
                break
        if cut_idx != -1: break

    if cut_idx > 5:
        main_text = "\n".join(lines[:cut_idx])
        attach_text = "\n".join(lines[cut_idx:])
        return main_text.strip(), attach_text.strip()
    return text.strip(), ""

def convert_japanese_year(text: str) -> str:
    def replacer(match):
        era = match.group(1)
        year_str = match.group(2)
        year = 1 if year_str == "å…ƒ" else int(year_str)
        if era == "ä»¤å’Œ": west_year = 2018 + year
        elif era == "å¹³æˆ": west_year = 1988 + year
        elif era == "æ˜­å’Œ": west_year = 1925 + year
        else: return match.group(0)
        return f"{match.group(0)}ï¼ˆ{west_year}å¹´ï¼‰"
    return re.sub(r"(ä»¤å’Œ|å¹³æˆ|æ˜­å’Œ)\s*([0-9å…ƒ]+)\s*å¹´", replacer, text)

def guess_title(text: str, fallback: str) -> str:
    for l in text.splitlines()[:50]:
        s = l.strip()
        if 6 <= len(s) <= 120 and not re.match(r"^[\d\-\s\(\)]+$", s): return s
    return fallback

def guess_date(text: str) -> str:
    m = re.search(r"(ä»¤å’Œ|å¹³æˆ|æ˜­å’Œ)\s*[0-9å…ƒ]+\s*å¹´\s*\d+\s*æœˆ\s*\d+\s*æ—¥(ï¼ˆ\d{4}å¹´ï¼‰)?", text)
    if m: return m.group(0)
    m2 = re.search(r"\d{4}\s*å¹´\s*\d{1,2}\s*æœˆ\s*\d{1,2}\s*æ—¥", text)
    return m2.group(0) if m2 else ""

def guess_issuer(text: str) -> str:
    for cand in ["æ¶ˆé˜²åº", "ç·å‹™çœæ¶ˆé˜²åº", "æ¶ˆé˜²å±€", "å±é™ºç‰©ä¿å®‰å®¤", "äºˆé˜²èª²"]:
        if cand in text: return cand
    return ""

def tag_text(text: str) -> Tuple[List[str], List[str], Dict[str, List[str]]]:
    ev: Dict[str, List[str]] = {}; fac: List[str] = []; work: List[str] = []
    target = text[:8000]
    for t, ps in FACILITY_TAGS.items():
        if hits := [p for p in ps if re.search(p, target)]:
            fac.append(t); ev[t] = hits[:3]
    for t, ps in WORK_TAGS.items():
        if hits := [p for p in ps if re.search(p, target)]:
            work.append(t); ev[t] = hits[:3]
    if not fac and re.search(r"å±é™ºç‰©|æ¶ˆé˜²æ³•", target): fac.append("å…±é€š")
    return fac, work, ev

def make_summary(main_text: str, n: int) -> str:
    s = re.sub(r"\s+", " ", main_text.strip())
    return s[:n] + ("â€¦" if len(s) > n else "")

_ILLEGAL_CHARS_RE = re.compile(r"[\x00-\x08\x0b\x0c\x0e-\x1f]")

def _xls_safe(s) -> str:
    """Excelã«æ›¸ãè¾¼ã‚ãªã„åˆ¶å¾¡æ–‡å­—ã‚’é™¤å»ã™ã‚‹"""
    if not isinstance(s, str):
        return s
    return _ILLEGAL_CHARS_RE.sub("", s)

def write_excel_index(outdir: str, records: List[Record]):
    if not openpyxl: return
    wb = openpyxl.Workbook()
    ws = wb.active
    ws.title = "Index"
    ws.append(["ã‚¿ã‚¤ãƒˆãƒ«(æ¨å®š)", "æ—¥ä»˜(æ¨å®š)", "ç™ºå‡ºè€…(æ¨å®š)", "æ–½è¨­ã‚¿ã‚°", "æ¥­å‹™ã‚¿ã‚°", "needs_review", "ç†ç”±", "æ¦‚è¦(å…ˆé ­)", "å…ƒãƒ•ã‚¡ã‚¤ãƒ«"])
    for r in records:
        ws.append([_xls_safe(r.title_guess), _xls_safe(r.date_guess), _xls_safe(r.issuer_guess), " / ".join(r.tags_facility), " / ".join(r.tags_work), "TRUE" if r.needs_review else "FALSE", _xls_safe(r.reason), _xls_safe(r.summary), _xls_safe(r.relpath)])
    
    excel_path = os.path.join(outdir, "00_çµ±åˆç›®æ¬¡.xlsx")
    try:
        wb.save(excel_path)
    except PermissionError:
        raise PermissionError("00_çµ±åˆç›®æ¬¡.xlsx ãŒä»–ã®ã‚¢ãƒ—ãƒªã§é–‹ã‹ã‚Œã¦ã„ã¾ã™ã€‚é–‰ã˜ã¦ã‹ã‚‰ã‚„ã‚Šç›´ã—ã¦ãã ã•ã„ã€‚")

def write_md_indices(outdir: str, records: List[Record]):
    with open(os.path.join(outdir, "00_çµ±åˆç›®æ¬¡.md"), "w", encoding="utf-8") as f:
        f.write("# çµ±åˆç›®æ¬¡ï¼ˆæ¦‚è¦ä»˜ãï¼‰\n\n")
        for r in records:
            f.write(f"- **{r.title_guess}**\n  - æ—¥ä»˜: {r.date_guess} / ç™ºå‡º: {r.issuer_guess}\n  - ã‚¿ã‚°: [{'/'.join(r.tags_facility)}] [{'/'.join(r.tags_work)}]\n  - æ¦‚è¦: {r.summary}\n  - å…ƒ: `{r.relpath}`\n\n")

def write_binded_texts(outdir: str, records: List[Record], limit_bytes: int):
    chunk_idx = 1
    current_size = 0
    current_lines = []
    
    def flush():
        nonlocal chunk_idx, current_size, current_lines
        if not current_lines: return
        with open(os.path.join(outdir, f"NotebookLMç”¨_çµ±åˆãƒ‡ãƒ¼ã‚¿_{chunk_idx:02d}.txt"), "w", encoding="utf-8") as f:
            f.write("\n".join(current_lines))
        chunk_idx += 1
        current_size = 0
        current_lines = []

    for r in records:
        if not r.full_text_for_bind.strip(): continue
        block = f"\n\n{'='*60}\nã€DOCUMENT STARTã€‘\nå…ƒãƒ•ã‚¡ã‚¤ãƒ«: {r.relpath}\næŠ½å‡ºæ–¹å¼: {r.method}\n{'-'*60}\n{r.full_text_for_bind}\n{'='*60}\n\n"
        b_len = len(block.encode("utf-8"))
        if current_size + b_len > limit_bytes and current_size > 0: flush()
        current_lines.append(block)
        current_size += b_len
    flush()

def compute_sha1(path: str) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã®SHA1ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—ã—ã¦é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡ºã«ä½¿ã†"""
    h = hashlib.sha1()
    try:
        with open(get_safe_path(path), "rb") as f:
            for chunk in iter(lambda: f.read(65536), b""):
                h.update(chunk)
        return h.hexdigest()
    except Exception:
        return ""

def extract_txt(path: str) -> Tuple[str, str]:
    """ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ï¼ˆæ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚’è‡ªå‹•åˆ¤å®šï¼‰"""
    for enc in ("utf-8-sig", "cp932", "utf-8", "latin-1"):
        try:
            with open(get_safe_path(path), "r", encoding=enc, errors="ignore") as f:
                return f.read(), "txt_read"
        except Exception:
            continue
    return "", "txt_err"

def extract_csv(path: str) -> Tuple[str, str]:
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’Markdownè¡¨å½¢å¼ã«æ•´å½¢ã™ã‚‹"""
    for enc in ("utf-8-sig", "cp932", "utf-8"):
        try:
            with open(get_safe_path(path), "r", encoding=enc, newline="", errors="ignore") as f:
                rows = list(csv.reader(f))
            if not rows:
                return "", "csv_empty"
            out = []
            for row in rows[:400]:
                if any(c.strip() for c in row):
                    out.append("| " + " | ".join([c.strip().replace("\n", " ") for c in row]) + " |")
            return "\n".join(out), "csv_md"
        except Exception:
            continue
    return "", "csv_err"

def write_html_report(outdir: str, records: List[Record]):
    """äººé–“ãŒè¦‹ã‚„ã™ã„HTMLãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ã§é–‹ãã ã‘ã§OKï¼‰"""
    def esc(s: object) -> str:
        return _html.escape(str(s) if s is not None else "")

    total = len(records)
    ok_count = sum(1 for r in records if not r.needs_review)
    needs_rev_count = total - ok_count

    # æŠ½å‡ºæ–¹å¼ã”ã¨ã®é›†è¨ˆ
    method_counts: Dict[str, int] = {}
    for r in records:
        method_counts[r.method] = method_counts.get(r.method, 0) + 1
    method_rows = "".join(
        f"<tr><td>{esc(m)}</td><td style='text-align:right'>{c}</td></tr>"
        for m, c in sorted(method_counts.items(), key=lambda x: -x[1])
    )

    # æ–½è¨­ã‚¿ã‚°ãƒ»æ¥­å‹™ã‚¿ã‚°ç”¨ã®ãƒãƒƒã‚¸è‰²ãƒãƒƒãƒ—
    FAC_COLOR  = "#2563eb"
    WORK_COLOR = "#16a34a"

    def make_badge(text: str, color: str) -> str:
        return f'<span class="badge" style="background:{color}">{esc(text)}</span>'

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚«ãƒ¼ãƒ‰ç”Ÿæˆ
    cards_html = []
    for r in records:
        card_cls  = "card-review" if r.needs_review else "card-ok"
        rev_badge = '<span class="rev-badge">âš  è¦ç¢ºèª</span>' if r.needs_review else \
                    '<span class="ok-badge">âœ“ æ­£å¸¸</span>'
        fac_badges  = "".join(make_badge(t, FAC_COLOR)  for t in r.tags_facility)
        work_badges = "".join(make_badge(t, WORK_COLOR) for t in r.tags_work)
        tags_html   = (fac_badges + work_badges) or '<span style="color:#94a3b8;font-size:12px">ã‚¿ã‚°ãªã—</span>'

        date_str   = esc(r.date_guess)   or "æ—¥ä»˜ä¸æ˜"
        issuer_str = esc(r.issuer_guess) or "ç™ºå‡ºè€…ä¸æ˜"
        pages_str  = f"/{r.pages}p" if r.pages else ""
        method_str = esc(r.method)
        size_kb    = f"{r.size // 1024:,} KB" if r.size >= 1024 else f"{r.size} B"

        reason_html = (
            f'<div class="reason-box">âš  {esc(r.reason)}</div>'
            if r.reason else ""
        )

        # data-search ã«æ¤œç´¢å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¨éƒ¨ã¾ã¨ã‚ã‚‹ï¼ˆå°æ–‡å­—åŒ–ã¯JSå´ã§è¡Œã†ï¼‰
        search_data = " ".join([
            r.title_guess, r.summary, r.relpath,
            r.date_guess, r.issuer_guess,
            " ".join(r.tags_facility), " ".join(r.tags_work),
            r.reason, r.method,
        ]).replace('"', '')

        cards_html.append(f"""
<div class="card {card_cls}" data-search="{esc(search_data.lower())}">
  <div class="card-header">
    <div class="card-title">{esc(r.title_guess)}</div>
    {rev_badge}
  </div>
  <div class="meta">
    <span>ğŸ“… {date_str}</span>
    <span>ğŸ¢ {issuer_str}</span>
    <span>ğŸ“„ {esc(r.ext.upper().lstrip('.'))}{pages_str} Â· {size_kb}</span>
    <span class="method-tag">æŠ½å‡º: {method_str}</span>
  </div>
  <div class="tags">{tags_html}</div>
  <div class="summary">{esc(r.summary) or '<i style="color:#94a3b8">æœ¬æ–‡ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ</i>'}</div>
  <div class="filepath">ğŸ“ {esc(r.relpath)}</div>
  {reason_html}
</div>""")

    html_content = f"""<!DOCTYPE html>
<html lang="ja">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1.0">
<title>NoticeForge å‡¦ç†ãƒ¬ãƒãƒ¼ãƒˆ</title>
<style>
*{{box-sizing:border-box;margin:0;padding:0}}
body{{font-family:'Meiryo UI','Yu Gothic UI','Hiragino Sans',sans-serif;background:#f1f5f9;color:#1e293b;font-size:14px}}
/* â”€â”€â”€ ãƒ˜ãƒƒãƒ€ãƒ¼ â”€â”€â”€ */
.header{{background:linear-gradient(135deg,#1e40af,#2563eb);color:white;padding:24px 32px;display:flex;justify-content:space-between;align-items:flex-end;flex-wrap:wrap;gap:8px}}
.header h1{{font-size:22px;font-weight:bold}}
.header .sub{{opacity:.75;font-size:13px;margin-top:4px}}
/* â”€â”€â”€ çµ±è¨ˆãƒãƒ¼ â”€â”€â”€ */
.stats-bar{{background:white;border-bottom:1px solid #e2e8f0;padding:16px 32px;display:flex;gap:12px;flex-wrap:wrap;align-items:center}}
.stat-box{{background:#f8fafc;border:1px solid #e2e8f0;border-radius:8px;padding:10px 20px;text-align:center;min-width:100px}}
.stat-box .num{{font-size:26px;font-weight:bold;color:#1e40af}}
.stat-box .lbl{{font-size:11px;color:#64748b;margin-top:2px}}
.stat-box.warn .num{{color:#dc2626}}
.stat-box.good .num{{color:#16a34a}}
.method-table{{margin-left:auto;font-size:12px;border-collapse:collapse}}
.method-table td{{padding:2px 8px;border-bottom:1px solid #f1f5f9}}
.method-table tr:last-child td{{border-bottom:none}}
/* â”€â”€â”€ ã‚«ãƒ¼ãƒ‰ä¸€è¦§ â”€â”€â”€ */
.container{{max-width:1080px;margin:24px auto;padding:0 16px}}
/* â”€â”€â”€ æ¤œç´¢ãƒãƒ¼ â”€â”€â”€ */
.search-bar{{background:white;padding:12px 32px;border-bottom:1px solid #e2e8f0;display:flex;align-items:center;gap:12px;position:sticky;top:0;z-index:100;box-shadow:0 2px 6px rgba(0,0,0,.06)}}
.search-input{{flex:1;max-width:680px;padding:10px 16px 10px 42px;border:2px solid #e2e8f0;border-radius:8px;font-size:14px;font-family:inherit;outline:none;transition:border-color .2s;background:url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='18' height='18' fill='none' stroke='%2394a3b8' stroke-width='2' viewBox='0 0 24 24'%3E%3Ccircle cx='11' cy='11' r='8'/%3E%3Cpath d='m21 21-4.35-4.35'/%3E%3C/svg%3E") no-repeat 12px center}}
.search-input:focus{{border-color:#2563eb}}
.search-hint{{font-size:12px;color:#94a3b8}}
.search-count{{font-size:13px;color:#64748b;font-weight:bold;white-space:nowrap}}
.no-results{{text-align:center;padding:64px 16px;color:#94a3b8;font-size:15px;display:none}}
.card{{background:white;border-radius:10px;padding:18px 22px;margin-bottom:14px;border-left:5px solid #94a3b8;box-shadow:0 1px 4px rgba(0,0,0,.07);transition:box-shadow .2s}}
.card:hover{{box-shadow:0 3px 10px rgba(0,0,0,.12)}}
.card-ok{{border-left-color:#16a34a}}
.card-review{{border-left-color:#dc2626}}
.card-header{{display:flex;justify-content:space-between;align-items:flex-start;gap:12px;margin-bottom:10px}}
.card-title{{font-size:15px;font-weight:bold;color:#0f172a;line-height:1.5;flex:1}}
.ok-badge{{background:#dcfce7;color:#16a34a;border:1px solid #86efac;border-radius:6px;padding:2px 10px;font-size:12px;font-weight:bold;white-space:nowrap}}
.rev-badge{{background:#fee2e2;color:#dc2626;border:1px solid #fca5a5;border-radius:6px;padding:2px 10px;font-size:12px;font-weight:bold;white-space:nowrap}}
.meta{{display:flex;gap:14px;flex-wrap:wrap;color:#64748b;font-size:12px;margin-bottom:10px}}
.method-tag{{color:#94a3b8;font-size:11px}}
.tags{{display:flex;gap:6px;flex-wrap:wrap;margin-bottom:12px}}
.badge{{color:white;padding:2px 10px;border-radius:12px;font-size:12px;font-weight:500}}
.summary{{background:#f8fafc;border:1px solid #e2e8f0;border-radius:6px;padding:10px 14px;font-size:13px;line-height:1.75;color:#334155;max-height:150px;overflow-y:auto;margin-bottom:10px;white-space:pre-wrap}}
.filepath{{font-size:11px;color:#94a3b8;font-family:'Consolas','Courier New',monospace;word-break:break-all}}
.reason-box{{margin-top:8px;font-size:12px;color:#92400e;background:#fffbeb;border:1px solid #fde68a;border-radius:5px;padding:6px 12px}}
/* â”€â”€â”€ ãƒ•ãƒƒã‚¿ãƒ¼ â”€â”€â”€ */
.footer{{text-align:center;color:#94a3b8;font-size:11px;padding:24px;margin-top:8px}}
</style>
</head>
<body>
<div class="header">
  <div>
    <h1>NoticeForge å‡¦ç†ãƒ¬ãƒãƒ¼ãƒˆ</h1>
    <div class="sub">ç”Ÿæˆæ—¥æ™‚: {time.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}</div>
  </div>
</div>
<div class="stats-bar">
  <div class="stat-box"><div class="num">{total}</div><div class="lbl">ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°</div></div>
  <div class="stat-box good"><div class="num">{ok_count}</div><div class="lbl">æ­£å¸¸æŠ½å‡º</div></div>
  <div class="stat-box warn"><div class="num">{needs_rev_count}</div><div class="lbl">è¦ç¢ºèª</div></div>
  <table class="method-table">
    <tr><td colspan="2" style="font-weight:bold;padding-bottom:4px">æŠ½å‡ºæ–¹å¼åˆ¥</td></tr>
    {method_rows}
  </table>
</div>
<div class="search-bar">
  <input class="search-input" id="searchInput" type="text"
    placeholder="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§çµã‚Šè¾¼ã‚€ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãƒ»ç™ºå‡ºè€…ãƒ»ãƒ•ã‚¡ã‚¤ãƒ«åãªã©ã€‚NotebookLMã®å¼•ç”¨æ–‡ã‚’ãã®ã¾ã¾è²¼ã‚Šä»˜ã‘ã¦ã‚‚OKï¼‰"
    oninput="filterCards()">
  <span class="search-hint">â†’ å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç´ æ—©ãæ¢ã›ã¾ã™</span>
  <span class="search-count" id="searchCount"></span>
</div>
<div class="container">
{''.join(cards_html)}
  <div class="no-results" id="noResults">è©²å½“ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚åˆ¥ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è©¦ã—ã¦ãã ã•ã„ã€‚</div>
</div>
<div class="footer">NoticeForge &mdash; NotebookLM é€£æºãƒ„ãƒ¼ãƒ«</div>
<script>
function filterCards() {{
  var q = document.getElementById('searchInput').value.toLowerCase();
  var cards = document.querySelectorAll('.card');
  var shown = 0;
  cards.forEach(function(card) {{
    var text = card.getAttribute('data-search');
    var match = !q || text.includes(q);
    card.style.display = match ? '' : 'none';
    if (match) shown++;
  }});
  var countEl = document.getElementById('searchCount');
  var noRes   = document.getElementById('noResults');
  countEl.textContent = q ? (shown + ' ä»¶ / ' + cards.length + ' ä»¶ä¸­') : (cards.length + ' ä»¶');
  noRes.style.display = (q && shown === 0) ? 'block' : 'none';
}}
window.addEventListener('load', function() {{
  document.getElementById('searchCount').textContent = document.querySelectorAll('.card').length + ' ä»¶';
}});
</script>
</body>
</html>"""

    with open(os.path.join(outdir, "00_äººé–“ç”¨ãƒ¬ãƒãƒ¼ãƒˆ.html"), "w", encoding="utf-8") as f:
        f.write(html_content)


def process_folder(indir: str, outdir: str, cfg: Dict[str, object], progress_callback: Optional[Callable[[int, int, str, str], None]] = None) -> Tuple[int, int, str]:
    os.makedirs(outdir, exist_ok=True)

    # â‘  å‰å›ã®ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ï¼ˆå¤ã„ãƒ‡ãƒ¼ã‚¿ãŒNotebookLMã«æ··å…¥ã—ãªã„ã‚ˆã†ã«ï¼‰
    for fname in os.listdir(outdir):
        if fname.startswith("NotebookLMç”¨_çµ±åˆãƒ‡ãƒ¼ã‚¿_") and fname.endswith(".txt"):
            try: os.remove(os.path.join(outdir, fname))
            except Exception: pass
    for fname in ("00_çµ±åˆç›®æ¬¡.md", "00_çµ±åˆç›®æ¬¡.xlsx", "00_äººé–“ç”¨ãƒ¬ãƒãƒ¼ãƒˆ.html", "00_å‡¦ç†ãƒ­ã‚°.txt"):
        p = os.path.join(outdir, fname)
        if os.path.exists(p):
            try: os.remove(p)
            except Exception: pass

    max_depth = int(cfg.get("max_depth", 30))
    split_kws = list(cfg.get("main_attach_split_keywords", []))
    min_chars = int(cfg.get("min_chars_mainbody", 800))
    use_ocr = bool(cfg.get("use_ocr", False))
    limit_bytes = int(cfg.get("bind_bytes_limit", 15000000))

    # ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é™¤å¤–ã™ã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    SKIP_FILENAMES = frozenset({"thumbs.db", "desktop.ini", ".ds_store"})
    SKIP_EXTENSIONS = frozenset({".db", ".tmp", ".bak", ".lnk", ".ini", ".cache"})

    targets = [
        os.path.join(root, fn)
        for root, _, files in os.walk(indir)
        if os.path.relpath(root, indir).count(os.sep) < max_depth
        for fn in files
        if fn.lower() not in SKIP_FILENAMES
        and os.path.splitext(fn)[1].lower() not in SKIP_EXTENSIONS
        and not fn.startswith("~$")
    ]
    total_files = len(targets)
    records: List[Record] = []

    # â‘£ SHA1 é‡è¤‡æ¤œå‡ºç”¨
    seen_sha1: set = set()
    skipped_dup = 0

    # â‘¥ å‡¦ç†ãƒ­ã‚°
    log_lines: List[str] = [
        "=== NoticeForge å‡¦ç†ãƒ­ã‚° ===",
        f"å‡¦ç†æ—¥æ™‚: {time.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}",
        f"å…¥åŠ›ãƒ•ã‚©ãƒ«ãƒ€: {indir}",
        f"å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€: {outdir}",
        "",
        "--- å„ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†çµæœ ---",
    ]

    for i, path in enumerate(targets):
        rel = os.path.relpath(path, indir)
        ext = os.path.splitext(path)[1].lower()
        if progress_callback: progress_callback(i + 1, total_files, rel, "(æŠ½å‡ºä¸­...)")

        # â‘£ SHA1 é‡è¤‡ãƒã‚§ãƒƒã‚¯
        sha1 = compute_sha1(path)
        if sha1 and sha1 in seen_sha1:
            if progress_callback: progress_callback(i + 1, total_files, rel, "(é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ã‚¹ã‚­ãƒƒãƒ—)")
            log_lines.append(f"[é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—] {rel}")
            skipped_dup += 1
            continue
        if sha1:
            seen_sha1.add(sha1)

        text, method, reason, pages = "", "unhandled", "", None

        try:
            if ext == ".pdf":
                if use_ocr and progress_callback: progress_callback(i + 1, total_files, rel, "(OCRå‡¦ç†ä¸­...æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™)")
                text, pages, method = extract_pdf(path, use_ocr)
            elif ext == ".docx":
                text, method = extract_docx(path)
            elif ext in (".xlsx", ".xlsm", ".xls"):
                text, method = extract_excel(path)
            elif ext in (".xdw", ".xbd"):
                text, method = extract_xdw(path)
            elif ext == ".txt":                          # â‘¢ .txt å¯¾å¿œ
                text, method = extract_txt(path)
            elif ext == ".csv":                          # â‘¢ .csv å¯¾å¿œ
                text, method = extract_csv(path)
        except Exception as e:
            method, reason = "error", f"æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e.__class__.__name__}"

        text = convert_japanese_year(text)
        main, attach = split_main_attach(text, split_kws)
        title = guess_title(main or text, os.path.basename(path))
        date_guess = guess_date(text)
        issuer_guess = guess_issuer(text)
        fac, work, ev = tag_text(main or text)

        needs_rev = False
        if method in ("unhandled", "error") or "missing" in method or len(main or text) < min_chars:
            needs_rev = True
            reason = reason or "æœ¬æ–‡ãŒçŸ­ã™ãã‚‹ã€ã¾ãŸã¯ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«"

        summary = make_summary(main or text, int(cfg.get("summary_chars", 900)))
        payload = f"ã‚¿ã‚¤ãƒˆãƒ«(æ¨å®š): {title}\næ—¥ä»˜(æ¨å®š): {date_guess}\nç™ºå‡ºè€…(æ¨å®š): {issuer_guess}\n\n# æœ¬æ–‡\n{main.strip()}"
        if attach.strip(): payload += f"\n\n# æ·»ä»˜è³‡æ–™\n{attach.strip()}"

        log_lines.append(f"[{method}] {rel}")
        if reason:
            log_lines.append(f"  â†’ {reason}")

        records.append(Record(relpath=rel, ext=ext, size=os.path.getsize(get_safe_path(path)), mtime=os.path.getmtime(get_safe_path(path)), sha1=sha1, method=method, pages=pages, text_chars=len(text), needs_review=needs_rev, reason=reason, title_guess=title, date_guess=date_guess, issuer_guess=issuer_guess, summary=summary, tags_facility=fac, tags_work=work, tag_evidence=ev, out_txt="", full_text_for_bind=payload))

    write_excel_index(outdir, records)
    write_md_indices(outdir, records)
    write_binded_texts(outdir, records, limit_bytes)
    write_html_report(outdir, records)

    # â‘¥ ã‚µãƒãƒªãƒ¼ã‚’é›†è¨ˆã—ã¦ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    needs_rev_count = len([r for r in records if r.needs_review])
    review_breakdown: Dict[str, int] = {}
    for r in records:
        if r.needs_review:
            key = r.method if ("missing" in r.method or r.method in ("unhandled", "error")) else "æœ¬æ–‡ãŒçŸ­ã™ãã‚‹"
            review_breakdown[key] = review_breakdown.get(key, 0) + 1

    log_lines += [
        "",
        "--- ã‚µãƒãƒªãƒ¼ ---",
        f"ç·å‡¦ç†æ•°: {len(records)} ä»¶",
        f"æ­£å¸¸æŠ½å‡º: {len(records) - needs_rev_count} ä»¶",
        f"è¦ç¢ºèª: {needs_rev_count} ä»¶",
    ]
    for k, v in sorted(review_breakdown.items(), key=lambda x: -x[1]):
        log_lines.append(f"  ãƒ»{k}: {v} ä»¶")
    if skipped_dup:
        log_lines.append(f"é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {skipped_dup} ä»¶")

    with open(os.path.join(outdir, "00_å‡¦ç†ãƒ­ã‚°.txt"), "w", encoding="utf-8") as f:
        f.write("\n".join(log_lines))

    # â‘¥ GUI ã«æ¸¡ã™å†…è¨³æ–‡å­—åˆ—
    breakdown_str = "ã€€".join(f"{k}: {v}ä»¶" for k, v in sorted(review_breakdown.items(), key=lambda x: -x[1]))
    return len(records), needs_rev_count, breakdown_str
